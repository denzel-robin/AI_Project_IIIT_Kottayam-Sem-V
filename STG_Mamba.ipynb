{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### prepare.py\n",
        "\n",
        "import torch.utils.data as utils\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def PrepareDataset(speed_matrix, BATCH_SIZE=48, seq_len=12, pred_len=12, train_propotion=0.7, valid_propotion=0.1):\n",
        "    time_len = speed_matrix.shape[0]\n",
        "\n",
        "    # MinMax Normalization Method.\n",
        "    max_speed = speed_matrix.max().max()\n",
        "    min_speed = speed_matrix.min().min()\n",
        "    speed_matrix = (speed_matrix - min_speed)/(max_speed - min_speed)\n",
        "\n",
        "    speed_sequences, speed_labels = [], []\n",
        "    for i in range(time_len - seq_len - pred_len):\n",
        "        speed_sequences.append(speed_matrix.iloc[i:i + seq_len].values)\n",
        "        speed_labels.append(speed_matrix.iloc[i + seq_len:i + seq_len + pred_len].values)\n",
        "    speed_sequences, speed_labels = np.asarray(speed_sequences), np.asarray(speed_labels)\n",
        "\n",
        "    # Reshape labels to have the same second dimension as the sequences\n",
        "    speed_labels = speed_labels.reshape(speed_labels.shape[0], seq_len, -1)\n",
        "\n",
        "    # shuffle & split the dataset to training and testing sets\n",
        "    sample_size = speed_sequences.shape[0]\n",
        "    index = np.arange(sample_size, dtype=int)\n",
        "    np.random.shuffle(index)\n",
        "\n",
        "    train_index = int(np.floor(sample_size * train_propotion))\n",
        "    valid_index = int(np.floor(sample_size * (train_propotion + valid_propotion)))\n",
        "\n",
        "    train_data, train_label = speed_sequences[:train_index], speed_labels[:train_index]\n",
        "    valid_data, valid_label = speed_sequences[train_index:valid_index], speed_labels[train_index:valid_index]\n",
        "    test_data, test_label = speed_sequences[valid_index:], speed_labels[valid_index:]\n",
        "\n",
        "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
        "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
        "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
        "\n",
        "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
        "    valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
        "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
        "\n",
        "    train_dataloader = utils.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    valid_dataloader = utils.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    test_dataloader = utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader, max_speed"
      ],
      "metadata": {
        "id": "Le-odSqAnKSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### modules.py\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "\n",
        "\n",
        "class DynamicFilterGNN(nn.Module):\n",
        "    def __init__(self, in_features, out_features, filter_adjacency_matrix, bias=True):\n",
        "        super(DynamicFilterGNN, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.base_filter = nn.Parameter(torch.Tensor(in_features, in_features))\n",
        "\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        self.filter_adjacency_matrix = None\n",
        "        if use_gpu:\n",
        "            self.filter_adjacency_matrix = Variable(filter_adjacency_matrix.cuda(), requires_grad=False)\n",
        "        else:\n",
        "            self.filter_adjacency_matrix = Variable(filter_adjacency_matrix, requires_grad=False)\n",
        "\n",
        "        self.transform = nn.Linear(in_features, in_features)\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.base_filter.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        transformed_filter = self.transform(self.base_filter)\n",
        "        transformed_adjacency = 0.9*self.filter_adjacency_matrix+0.1*transformed_filter\n",
        "        result_embed = F.linear(input, transformed_adjacency.matmul(self.weight), self.bias)\n",
        "        return result_embed\n",
        "\n",
        "\n",
        "    def get_transformed_adjacency(self):\n",
        "        transformed_filter = self.transform(self.base_filter)\n",
        "        transformed_adjacency = 0.9 * self.filter_adjacency_matrix + 0.1 * transformed_filter\n",
        "        return transformed_adjacency\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "               + 'in_features=' + str(self.in_features) \\\n",
        "               + ', out_features=' + str(self.out_features) \\\n",
        "               + ', bias=' + str(self.bias is not None) + ')'\n",
        "\n"
      ],
      "metadata": {
        "id": "uQFJtmPznlfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### STGMamba.py\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from einops import rearrange, repeat, einsum\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "# KFGN (Kalman Filtering Graph Neural Networks) Model\n",
        "class KFGN(nn.Module):\n",
        "    def __init__(self, K, A, feature_size, Clamp_A=True):\n",
        "        super(KFGN, self).__init__()\n",
        "        self.feature_size = feature_size\n",
        "        self.hidden_size = feature_size\n",
        "        self.K = K\n",
        "        self.A_list = []\n",
        "\n",
        "        D_inverse = torch.diag(1 / torch.sum(A, 0))\n",
        "        norm_A = torch.matmul(D_inverse, A)\n",
        "        A = norm_A\n",
        "\n",
        "        A_temp = torch.eye(feature_size, feature_size)\n",
        "        for i in range(K):\n",
        "            A_temp = torch.matmul(A_temp, A)\n",
        "            if Clamp_A:\n",
        "                A_temp = torch.clamp(A_temp, max=1.)\n",
        "            self.A_list.append(A_temp)\n",
        "\n",
        "        self.gc_list = nn.ModuleList([DynamicFilterGNN(feature_size, feature_size, self.A_list[i], bias=False) for i in range(K)])\n",
        "        hidden_size = self.feature_size\n",
        "        gc_input_size = self.feature_size * K\n",
        "\n",
        "        self.fl = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "        self.il = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "        self.ol = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "        self.Cl = nn.Linear(gc_input_size + hidden_size, hidden_size)\n",
        "\n",
        "\n",
        "        self.Neighbor_weight = Parameter(torch.FloatTensor(feature_size))\n",
        "        stdv = 1. / math.sqrt(feature_size)\n",
        "        self.Neighbor_weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "        input_size = self.feature_size\n",
        "\n",
        "        self.rfl = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.ril = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.rol = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.rCl = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        # addtional vars\n",
        "        self.c = torch.nn.Parameter(torch.Tensor([1]))\n",
        "\n",
        "        self.fc1 = nn.Linear(64, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, 64)\n",
        "        self.fc5 = nn.Linear(64, hidden_size)\n",
        "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc8 = nn.Linear(hidden_size, 64)\n",
        "\n",
        "    def forward(self, input, Hidden_State=None, Cell_State=None, rHidden_State=None, rCell_State=None):\n",
        "        batch_size, time_steps, _ = input.shape\n",
        "        if Hidden_State is None:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "        if Cell_State is None:\n",
        "            Cell_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "        if rHidden_State is None:\n",
        "            rHidden_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "        if rCell_State is None:\n",
        "            rCell_State = Variable(torch.zeros(batch_size,self.feature_size).cuda())\n",
        "\n",
        "        Hidden_State = Hidden_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "        Cell_State = Cell_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "        rHidden_State = rHidden_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "        rCell_State = rCell_State.unsqueeze(1).expand(-1, time_steps, -1)\n",
        "\n",
        "        x = input\n",
        "        gc = self.gc_list[0](x)\n",
        "        for i in range(1, self.K):\n",
        "            gc = torch.cat((gc, self.gc_list[i](x)), 1)\n",
        "\n",
        "        combined = torch.cat((gc, Hidden_State), 1)\n",
        "        dim1=combined.shape[0]\n",
        "        dim2=combined.shape[1]\n",
        "        dim3=combined.shape[2]\n",
        "        combined=combined.view(dim1,dim2//4,dim3*4)\n",
        "\n",
        "        f = torch.sigmoid(self.fl(combined))\n",
        "        i = torch.sigmoid(self.il(combined))\n",
        "        o = torch.sigmoid(self.ol(combined))\n",
        "        C = torch.tanh(self.Cl(combined))\n",
        "\n",
        "        NC = torch.mul(Cell_State,\n",
        "                       torch.mv(Variable(self.A_list[-1], requires_grad=False).cuda(), self.Neighbor_weight))\n",
        "        Cell_State = f * NC + i * C\n",
        "        Hidden_State = o * torch.tanh(Cell_State)\n",
        "\n",
        "        # LSTM\n",
        "        rcombined = torch.cat((input, rHidden_State), 1)\n",
        "        d1=rcombined.shape[0]\n",
        "        d2=rcombined.shape[1]\n",
        "        d3=rcombined.shape[2]\n",
        "        rcombined=rcombined.view(d1,d2//2,d3*2)\n",
        "        rf = torch.sigmoid(self.rfl(rcombined))\n",
        "        ri = torch.sigmoid(self.ril(rcombined))\n",
        "        ro = torch.sigmoid(self.rol(rcombined))\n",
        "        rC = torch.tanh(self.rCl(rcombined))\n",
        "        rCell_State = rf * rCell_State + ri * rC\n",
        "        rHidden_State = ro * torch.tanh(rCell_State)\n",
        "\n",
        "        # Kalman Filtering\n",
        "        var1, var2 = torch.var(input), torch.var(gc)\n",
        "\n",
        "        pred = (Hidden_State * var1 * self.c + rHidden_State * var2) / (var1 + var2 * self.c)\n",
        "\n",
        "        return pred\n",
        "        #return Hidden_State, Cell_State, gc, rHidden_State, rCell_State, pred\n",
        "\n",
        "    def Bi_torch(self, a):\n",
        "        a[a < 0] = 0\n",
        "        a[a > 0] = 1\n",
        "        return a\n",
        "\n",
        "    def loop(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        time_step = inputs.size(1)\n",
        "        Hidden_State, Cell_State, rHidden_State, rCell_State = self.initHidden(batch_size)\n",
        "        for i in range(time_step):\n",
        "            Hidden_State, Cell_State, gc, rHidden_State, rCell_State, pred = self.forward(\n",
        "                torch.squeeze(inputs[:, i:i + 1, :]), Hidden_State, Cell_State, rHidden_State, rCell_State)\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        if use_gpu:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            rHidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            rCell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "        else:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            rHidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            rCell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "\n",
        "    def reinitHidden(self, batch_size, Hidden_State_data, Cell_State_data):\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        if use_gpu:\n",
        "            Hidden_State = Variable(Hidden_State_data.cuda(), requires_grad=True)\n",
        "            Cell_State = Variable(Cell_State_data.cuda(), requires_grad=True)\n",
        "            rHidden_State = Variable(Hidden_State_data.cuda(), requires_grad=True)\n",
        "            rCell_State = Variable(Cell_State_data.cuda(), requires_grad=True)\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "        else:\n",
        "            Hidden_State = Variable(Hidden_State_data, requires_grad=True)\n",
        "            Cell_State = Variable(Cell_State_data, requires_grad=True)\n",
        "            rHidden_State = Variable(Hidden_State_data.cuda(), requires_grad=True)\n",
        "            rCell_State = Variable(Cell_State_data.cuda(), requires_grad=True)\n",
        "            return Hidden_State, Cell_State, rHidden_State, rCell_State\n",
        "\n",
        "# Mamba Network\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    d_model: int\n",
        "    n_layer: int\n",
        "    features: int\n",
        "    d_state: int = 16\n",
        "    expand: int = 2\n",
        "    dt_rank: Union[int, str] = 'auto'\n",
        "    d_conv: int = 4\n",
        "    conv_bias: bool = True\n",
        "    bias: bool = False\n",
        "    K: int = 3\n",
        "    A: torch.Tensor = None\n",
        "    feature_size: int = None\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = int(self.expand * self.d_model)\n",
        "        if self.dt_rank == 'auto':\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "class KFGN_Mamba(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.kfgn = KFGN(K=args.K, A=args.A, feature_size=args.feature_size)\n",
        "        self.encode = nn.Linear(args.features, args.d_model)\n",
        "        self.encoder_layers = nn.ModuleList([ResidualBlock(args,self.kfgn) for _ in range(args.n_layer)])\n",
        "        self.encoder_norm = RMSNorm(args.d_model)\n",
        "        self.decode = nn.Linear(args.d_model, args.features)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.encode(input_ids)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "        x = self.encoder_norm(x)\n",
        "\n",
        "        # Output\n",
        "        x = self.decode(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Residual Block in Mamba Model\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs, kfgn: KFGN):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.kfgn = KFGN(K=args.K, A=args.A, feature_size=args.feature_size)\n",
        "        self.mixer = MambaBlock(args,kfgn)\n",
        "        self.norm = RMSNorm(args.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x\n",
        "        x1 = self.norm(x)\n",
        "        x2 = self.kfgn(x1)\n",
        "        x3 = self.mixer(x2)\n",
        "        output = x3 + x1\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs, kfgn: KFGN):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.kfgn = kfgn\n",
        "\n",
        "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=args.d_inner,\n",
        "            out_channels=args.d_inner,\n",
        "            bias=args.conv_bias,\n",
        "            kernel_size=args.d_conv,\n",
        "            groups=args.d_inner,\n",
        "            padding=args.d_conv - 1,\n",
        "        )\n",
        "\n",
        "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
        "\n",
        "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
        "\n",
        "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
        "        self.A_log = nn.Parameter(torch.log(A))\n",
        "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
        "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        (b, l, d) = x.shape\n",
        "\n",
        "        x_and_res = self.in_proj(x)\n",
        "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
        "\n",
        "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
        "        x = self.conv1d(x)[:, :, :l]\n",
        "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
        "\n",
        "        x = F.silu(x)\n",
        "\n",
        "        y = self.ssm(x)\n",
        "\n",
        "        y = y * F.silu(res)\n",
        "\n",
        "        output = self.out_proj(y)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def ssm(self, x):\n",
        "        (d_in, n) = self.A_log.shape\n",
        "\n",
        "        A = -torch.exp(self.A_log.float())\n",
        "        D = self.D.float()\n",
        "\n",
        "        x_dbl = self.x_proj(x)\n",
        "\n",
        "        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)\n",
        "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
        "\n",
        "        y = self.selective_scan(x, delta, A, B, C, D)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "    def selective_scan(self, u, delta, A, B, C, D):\n",
        "        (b, l, d_in) = u.shape\n",
        "        n = A.shape[1]\n",
        "        temp_adj = self.kfgn.gc_list[-1].get_transformed_adjacency()\n",
        "        temp_adj_padded = torch.ones(d_in, d_in, device=temp_adj.device)\n",
        "        temp_adj_padded[:temp_adj.size(0), :temp_adj.size(1)] = temp_adj\n",
        "\n",
        "        delta_p = torch.matmul(delta,temp_adj_padded)\n",
        "\n",
        "        deltaA = torch.exp(einsum(delta_p, A, 'b l d_in, d_in n -> b l d_in n'))\n",
        "        deltaB_u = einsum(delta_p, B, u, 'b l d_in, b l n, b l d_in -> b l d_in n')\n",
        "\n",
        "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
        "        ys = []\n",
        "        for i in range(l):\n",
        "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
        "            y = einsum(x, C[:, i, :], 'b d_in n, b n -> b d_in')\n",
        "            ys.append(y)\n",
        "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
        "\n",
        "        y = y + u * D\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "2LGB4WYWnek0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### train_STGmamba.py\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.autograd import Variable\n",
        "\n",
        "y = []\n",
        "\n",
        "def TrainSTG_Mamba(train_dataloader, valid_dataloader, A, K=3, num_epochs=1, mamba_features=307):\n",
        "    inputs, labels = next(iter(train_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "    input_dim = fea_size\n",
        "    hidden_dim = fea_size\n",
        "    output_dim = fea_size\n",
        "\n",
        "    kfgn_mamba_args = ModelArgs(\n",
        "        K=K,\n",
        "        A=torch.Tensor(A),\n",
        "        feature_size=A.shape[0],\n",
        "        d_model=fea_size,\n",
        "        n_layer=4,\n",
        "        features=mamba_features\n",
        "    )\n",
        "\n",
        "    kfgn_mamba = KFGN_Mamba(kfgn_mamba_args)\n",
        "    kfgn_mamba.cuda()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    learning_rate = 1e-4\n",
        "    optimizer = optim.AdamW(kfgn_mamba.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, amsgrad=False)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    interval = 100\n",
        "    losses_train = []\n",
        "    losses_interval_train = []\n",
        "    losses_valid = []\n",
        "    losses_interval_valid = []\n",
        "    losses_epoch = []\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        trained_number = 0\n",
        "\n",
        "        valid_dataloader_iter = iter(valid_dataloader)\n",
        "\n",
        "        for data in train_dataloader:\n",
        "            inputs, labels = data\n",
        "\n",
        "            if inputs.shape[0] != batch_size:\n",
        "                continue\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "            else:\n",
        "                inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "            kfgn_mamba.zero_grad()\n",
        "\n",
        "            labels = torch.squeeze(labels)\n",
        "            pred = kfgn_mamba(inputs)\n",
        "\n",
        "            loss_train = loss_MSE(pred, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "            # Update learning rate by CosineAnnealingLR\n",
        "            scheduler.step()\n",
        "\n",
        "            losses_train.append(loss_train.data)\n",
        "\n",
        "            # validation\n",
        "            try:\n",
        "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "            except StopIteration:\n",
        "                valid_dataloader_iter = iter(valid_dataloader)\n",
        "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs_val, labels_val = Variable(inputs_val.cuda()), Variable(labels_val.cuda())\n",
        "            else:\n",
        "                inputs_val, labels_val = Variable(inputs_val), Variable(labels_val)\n",
        "\n",
        "            labels_val = torch.squeeze(labels_val)\n",
        "\n",
        "            pred = kfgn_mamba(inputs_val)\n",
        "            loss_valid = loss_MSE(pred, labels_val)\n",
        "            losses_valid.append(loss_valid.data)\n",
        "\n",
        "            trained_number += 1\n",
        "\n",
        "            if trained_number % interval == 0:\n",
        "                cur_time = time.time()\n",
        "                loss_interval_train = np.around(sum(losses_train[-interval:]).cpu().numpy() / interval, decimals=8)\n",
        "                losses_interval_train.append(loss_interval_train)\n",
        "                loss_interval_valid = np.around(sum(losses_valid[-interval:]).cpu().numpy() / interval, decimals=8)\n",
        "                losses_interval_valid.append(loss_interval_valid)\n",
        "                print('Iteration #: {}, train_loss: {}, valid_loss: {}, time: {}'.format(\n",
        "                    trained_number * batch_size,\n",
        "                    loss_interval_train,\n",
        "                    loss_interval_valid,\n",
        "                    np.around([cur_time - pre_time], decimals=8)))\n",
        "                pre_time = cur_time\n",
        "\n",
        "        loss_epoch = loss_valid.cpu().data.numpy()\n",
        "        losses_epoch.append(loss_epoch)\n",
        "\n",
        "    return kfgn_mamba, [losses_train, losses_interval_train, losses_valid, losses_interval_valid]\n",
        "\n",
        "\n",
        "\n",
        "def TestSTG_Mamba(kfgn_mamba, test_dataloader, max_speed):\n",
        "    inputs, labels = next(iter(test_dataloader))\n",
        "    [batch_size, step_size, fea_size] = inputs.size()\n",
        "\n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    tested_batch = 0\n",
        "\n",
        "    losses_mse = []\n",
        "    losses_l1 = []\n",
        "    MAEs = []\n",
        "    MAPEs = []\n",
        "    MSEs = []\n",
        "    RMSEs = []\n",
        "    VARs = []\n",
        "\n",
        "    for data in test_dataloader:\n",
        "        inputs, labels = data\n",
        "\n",
        "        if inputs.shape[0] != batch_size:\n",
        "            continue\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "        else:\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "        pred = kfgn_mamba(inputs)\n",
        "        labels = torch.squeeze(labels)\n",
        "\n",
        "        loss_mse = F.mse_loss(pred, labels)\n",
        "        loss_l1 = F.l1_loss(pred, labels)\n",
        "        MAE = torch.mean(torch.abs(pred - torch.squeeze(labels)))\n",
        "        MAPE = torch.mean(torch.abs(pred - torch.squeeze(labels)) / torch.abs(torch.squeeze(labels)))\n",
        "        # Calculate MAPE only for non-zero labels\n",
        "        non_zero_labels = torch.abs(labels) > 0\n",
        "        if torch.any(non_zero_labels):\n",
        "            MAPE_values = torch.abs(pred - torch.squeeze(labels)) / torch.abs(torch.squeeze(labels))\n",
        "            MAPE = torch.mean(MAPE_values[non_zero_labels])\n",
        "            MAPEs.append(MAPE.item())\n",
        "\n",
        "        MSE = torch.mean((torch.squeeze(labels) - pred)**2)\n",
        "        RMSE = math.sqrt(torch.mean((torch.squeeze(labels) - pred)**2))\n",
        "        VAR = 1-(torch.var(torch.squeeze(labels)-pred))/torch.var(torch.squeeze(labels))\n",
        "\n",
        "        losses_mse.append(loss_mse.item())\n",
        "        losses_l1.append(loss_l1.item())\n",
        "        MAEs.append(MAE.item())\n",
        "        MAPEs.append(MAPE.item())\n",
        "        MSEs.append(MSE.item())\n",
        "        RMSEs.append(RMSE)\n",
        "        VARs.append(VAR.item())\n",
        "\n",
        "        y.append(pred.cpu().data.numpy())\n",
        "\n",
        "        tested_batch += 1\n",
        "\n",
        "        if tested_batch % 100 == 0:\n",
        "            cur_time = time.time()\n",
        "            print('Tested #: {}, loss_l1: {}, loss_mse: {}, time: {}'.format(\n",
        "                tested_batch * batch_size,\n",
        "                np.around([loss_l1.data[0]], decimals=8),\n",
        "                np.around([loss_mse.data[0]], decimals=8),\n",
        "                np.around([cur_time - pre_time], decimals=8)))\n",
        "            pre_time = cur_time\n",
        "\n",
        "    losses_l1 = np.array(losses_l1)\n",
        "    losses_mse = np.array(losses_mse)\n",
        "    MAEs = np.array(MAEs)\n",
        "    MAPEs = np.array(MAPEs)\n",
        "    MSEs = np.array(MSEs)\n",
        "    RMSEs = np.array(RMSEs)\n",
        "    VARs = np.array(VARs)\n",
        "\n",
        "    mean_l1 = np.mean(losses_l1) * max_speed\n",
        "    std_l1 = np.std(losses_l1) * max_speed\n",
        "    mean_mse = np.mean(losses_mse) * max_speed\n",
        "    MAE_ = np.mean(MAEs) * max_speed\n",
        "    std_MAE_ = np.std(MAEs) * max_speed\n",
        "    MAPE_ = np.mean(MAPEs) * 100\n",
        "    MSE_ = np.mean(MSEs) * (max_speed ** 2)\n",
        "    RMSE_ = np.mean(RMSEs) * max_speed\n",
        "    VAR_ = np.mean(VARs)\n",
        "    results = [MAE_, std_MAE_, MAPE_, MSE_, RMSE_, VAR_]\n",
        "\n",
        "    print('Tested: MAE: {}, std_MAE: {}, MAPE: {}, MSE: {}, RMSE: {}, VAR: {}'.format(MAE_, std_MAE_, MAPE_, MSE_, RMSE_, VAR_))\n",
        "    return results"
      ],
      "metadata": {
        "id": "AB_S2LDJnVWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### main.py\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "print(\"\\nLoading PEMS04 data...\")\n",
        "speed_matrix = pd.read_csv('pems04_flow.csv',sep=',')\n",
        "A = np.load('pems04_adj.npy')\n",
        "\n",
        "\n",
        "print(\"\\nPreparing train/test data...\")\n",
        "train_dataloader, valid_dataloader, test_dataloader, max_value = PrepareDataset(speed_matrix, BATCH_SIZE=48)\n",
        "\n",
        "print(\"\\nTraining STGmamba model...\")\n",
        "STGmamba, STGmamba_loss = TrainSTG_Mamba(train_dataloader, valid_dataloader, A, K=3, num_epochs=50, mamba_features=307)\n",
        "print(\"\\nTesting STGmamba model...\")\n",
        "results = TestSTG_Mamba(STGmamba, test_dataloader, max_value)"
      ],
      "metadata": {
        "id": "z4YRyEW8e_-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39f33d2-ebf8-4285-b8fc-66827c56b3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading PEMS04 data...\n",
            "\n",
            "Preparing train/test data...\n",
            "\n",
            "Training STGmamba model...\n",
            "Epoch 0/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.05558548867702484, valid_loss: 0.052441079169511795, time: [15.85999417]\n",
            "Epoch 1/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.008964519947767258, valid_loss: 0.009449600242078304, time: [15.16934872]\n",
            "Epoch 2/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.005803979933261871, valid_loss: 0.006208720151335001, time: [15.28930855]\n",
            "Epoch 3/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.00423581013455987, valid_loss: 0.0045500099658966064, time: [15.52523303]\n",
            "Epoch 4/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0034912601113319397, valid_loss: 0.0037530199624598026, time: [15.51936364]\n",
            "Epoch 5/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0031669000163674355, valid_loss: 0.003410120029002428, time: [16.01673698]\n",
            "Epoch 6/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002972719958052039, valid_loss: 0.0032074199989438057, time: [15.34675288]\n",
            "Epoch 7/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0028878599405288696, valid_loss: 0.0031272899359464645, time: [15.59968948]\n",
            "Epoch 8/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0027582200709730387, valid_loss: 0.003006220096722245, time: [15.43560433]\n",
            "Epoch 9/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0026492299512028694, valid_loss: 0.0029182101134210825, time: [15.37409353]\n",
            "Epoch 10/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0026228600181639194, valid_loss: 0.0028755399398505688, time: [15.5037601]\n",
            "Epoch 11/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002490780083462596, valid_loss: 0.002772059990093112, time: [15.66709542]\n",
            "Epoch 12/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002437409944832325, valid_loss: 0.0027247900143265724, time: [15.68612647]\n",
            "Epoch 13/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0023845299147069454, valid_loss: 0.0026757901068776846, time: [15.42922258]\n",
            "Epoch 14/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0023631800431758165, valid_loss: 0.002666760003194213, time: [15.8080883]\n",
            "Epoch 15/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0023109300527721643, valid_loss: 0.002625440014526248, time: [15.53854942]\n",
            "Epoch 16/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0022464198991656303, valid_loss: 0.0025786999613046646, time: [15.55458832]\n",
            "Epoch 17/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0022301399149000645, valid_loss: 0.0025738601107150316, time: [15.73271036]\n",
            "Epoch 18/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0021873300429433584, valid_loss: 0.002529559889808297, time: [15.58227468]\n",
            "Epoch 19/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0021344199776649475, valid_loss: 0.0025055198930203915, time: [15.54397321]\n",
            "Epoch 20/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0021272199228405952, valid_loss: 0.0024985899217426777, time: [15.66383696]\n",
            "Epoch 21/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0020816500764340162, valid_loss: 0.0024668399710208178, time: [15.7185936]\n",
            "Epoch 22/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0020798600744456053, valid_loss: 0.002472710097208619, time: [15.6345017]\n",
            "Epoch 23/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002018169965595007, valid_loss: 0.0024122800678014755, time: [15.75271845]\n",
            "Epoch 24/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.002003750065341592, valid_loss: 0.002407910069450736, time: [15.91569328]\n",
            "Epoch 25/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.001984710106626153, valid_loss: 0.002401249948889017, time: [15.92071486]\n",
            "Epoch 26/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019844900816679, valid_loss: 0.002405870007351041, time: [15.76252937]\n",
            "Epoch 27/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019516400061547756, valid_loss: 0.0023739200551062822, time: [16.11926222]\n",
            "Epoch 28/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019117899937555194, valid_loss: 0.0023303499910980463, time: [15.70297432]\n",
            "Epoch 29/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0019082200014963746, valid_loss: 0.0023525299038738012, time: [15.72666621]\n",
            "Epoch 30/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018925199983641505, valid_loss: 0.0023306100629270077, time: [16.05140924]\n",
            "Epoch 31/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018778899684548378, valid_loss: 0.002321680076420307, time: [15.77568483]\n",
            "Epoch 32/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018548499792814255, valid_loss: 0.0023175000678747892, time: [15.5970602]\n",
            "Epoch 33/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018427999457344413, valid_loss: 0.0022959900088608265, time: [15.42417383]\n",
            "Epoch 34/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0018288899445906281, valid_loss: 0.0023042999673634768, time: [15.48698568]\n",
            "Epoch 35/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.001795780030079186, valid_loss: 0.0022734899539500475, time: [15.43201804]\n",
            "Epoch 36/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017958399839699268, valid_loss: 0.0022809701040387154, time: [15.45179391]\n",
            "Epoch 37/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017635399708524346, valid_loss: 0.002246110001578927, time: [15.81989002]\n",
            "Epoch 38/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017805000534281135, valid_loss: 0.002269810065627098, time: [15.4115932]\n",
            "Epoch 39/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.00178418995346874, valid_loss: 0.002279059961438179, time: [15.49009609]\n",
            "Epoch 40/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.001732559991069138, valid_loss: 0.0022209100425243378, time: [15.58972692]\n",
            "Epoch 41/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017183000454679132, valid_loss: 0.0022175800986588, time: [15.44024348]\n",
            "Epoch 42/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0017019599908962846, valid_loss: 0.0022030100226402283, time: [15.47612405]\n",
            "Epoch 43/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016944200033321977, valid_loss: 0.0022004200145602226, time: [15.54832554]\n",
            "Epoch 44/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016774500254541636, valid_loss: 0.0021880699787288904, time: [15.5999198]\n",
            "Epoch 45/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016973799793049693, valid_loss: 0.0022216199431568384, time: [15.47956967]\n",
            "Epoch 46/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016742800362408161, valid_loss: 0.0021911400835961103, time: [15.49991584]\n",
            "Epoch 47/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016437199665233493, valid_loss: 0.0021699799690395594, time: [15.6702106]\n",
            "Epoch 48/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016300099669024348, valid_loss: 0.002162100048735738, time: [15.45214224]\n",
            "Epoch 49/49\n",
            "----------\n",
            "Iteration #: 4800, train_loss: 0.0016221200348809361, valid_loss: 0.002154489979147911, time: [15.47652555]\n",
            "\n",
            "Testing STGmamba model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3240926241.py:174: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  RMSE = math.sqrt(torch.mean((torch.squeeze(labels) - pred)**2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tested: MAE: 22.419649216442785, std_MAE: 0.8909896054798512, MAPE: 18.506053295628778, MSE: 1172.1527429045668, RMSE: 34.2167048183468, VAR: 0.9440122493382158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fi_AYn8nlDiZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}